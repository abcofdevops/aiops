## Running chatbot from local LLM model


### Steps to Run Application

[Application Run Guide](../README.md)

**Running Application**
  ```bash
  pip3 install -r requirements.txt
  python3 app.py
  ```


##  How It Works

1. The script starts the chat and take your input.
2. Connects to the Ollama API running locally.
3. Generates an Answer.
4. Returns the Answer as output.
5. It keeps the chat open until you type exit or quit.
